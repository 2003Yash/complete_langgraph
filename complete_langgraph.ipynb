{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGjPa7MRNtSVu4I4Jcf9Nc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/complete_langgraph/blob/main/complete_langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SIMPLE REACT AGENT FROM SCRATCH ( RE = REASONING + ACT)"
      ],
      "metadata": {
        "id": "fa4RiHQL0IyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple ReAct Agent Example\n",
        "# Concept: The agent Thinks -> Acts -> Observes -> Answers\n",
        "\n",
        "from openai import OpenAI\n",
        "import re\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Simple helper actions\n",
        "def calculate(expression):\n",
        "    \"\"\"Run a math calculation safely.\"\"\"\n",
        "    try:\n",
        "        return eval(expression)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def dog_weight(breed):\n",
        "    \"\"\"Return average weight of a dog breed.\"\"\"\n",
        "    weights = {\n",
        "        \"Bulldog\": 50,\n",
        "        \"Border Collie\": 37,\n",
        "        \"Scottish Terrier\": 20,\n",
        "        \"Toy Poodle\": 7\n",
        "    }\n",
        "    return f\"A {breed} weighs about {weights.get(breed, 50)} lbs\"\n",
        "\n",
        "# Map actions to functions\n",
        "actions = {\n",
        "    \"calculate\": calculate,\n",
        "    \"dog_weight\": dog_weight\n",
        "}\n",
        "\n",
        "# Simple prompt telling the model how to reason\n",
        "system_prompt = \"\"\"\n",
        "You think in a loop: Thought ‚Üí Action ‚Üí PAUSE ‚Üí Observation.\n",
        "End with: Answer.\n",
        "\n",
        "Available actions:\n",
        "- calculate: do math (example: calculate: 5 + 7)\n",
        "- dog_weight: get average dog weight (example: dog_weight: Bulldog)\n",
        "\"\"\"\n",
        "\n",
        "# Simple Agent class\n",
        "class Agent:\n",
        "    def __init__(self, system_prompt):\n",
        "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "    def step(self, user_input):\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "        reply = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0,\n",
        "            messages=self.messages\n",
        "        ).choices[0].message.content\n",
        "        print(reply)\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "        return reply\n",
        "\n",
        "# Regex to detect actions\n",
        "action_re = re.compile(r\"^Action: (\\w+): (.*)$\", re.MULTILINE)\n",
        "\n",
        "# Simple query loop\n",
        "def run_query(question):\n",
        "    agent = Agent(system_prompt)\n",
        "    user_input = question\n",
        "\n",
        "    for _ in range(5):  # limit to avoid infinite loops\n",
        "        reply = agent.step(user_input)\n",
        "        match = action_re.search(reply)\n",
        "        if match:\n",
        "            action_name, action_input = match.groups()\n",
        "            if action_name in actions:\n",
        "                observation = actions[action_name](action_input.strip())\n",
        "                user_input = f\"Observation: {observation}\"\n",
        "            else:\n",
        "                print(\"Unknown action:\", action_name)\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# Example run\n",
        "run_query(\"How much does a Toy Poodle weigh?\")\n",
        "\n",
        "\n",
        "# Agent Algorithm:\n",
        "\n",
        "# 1. Agent gets the question ‚Üí ‚ÄúHow much does a Toy Poodle weigh?‚Äù\n",
        "# 2. Thinks and decides to use an action:\n",
        "#   ‚Üí Action: dog_weight: Toy Poodle\n",
        "# 3. The code runs that action ‚Üí gets Observation: A Toy Poodle weighs about 7 lbs\n",
        "# 4. Agent is called again ‚Üí now gives the final Answer."
      ],
      "metadata": {
        "id": "A7jpl5nWJ7Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SIMPLE LANGGRAPH AGENT ( Exploring concepts of nodes, edges, conditional edges and tools )"
      ],
      "metadata": {
        "id": "Na6F9DaTKL_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Simple LangGraph Example\n",
        "# Concept: Agent uses Nodes, Edges, Tools, and Conditional Edges\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage # the agent‚Äôs conversation includes different roles ‚Äî user input, system instructions, AI replies, and tool results. Each message type helps the model understand who said what and manage context properly in the reasoning loop.\n",
        "import operator\n",
        "\n",
        "# --- TOOL (like a search or calculator) ---\n",
        "class SimpleSearchTool:\n",
        "    name = \"search_tool\"\n",
        "\n",
        "    def invoke(self, args):\n",
        "        query = args.get(\"query\", \"\")\n",
        "        return f\"Fake search results for: '{query}'\"\n",
        "\n",
        "tool = SimpleSearchTool()\n",
        "\n",
        "# --- STATE (holds the conversation) ---\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]\n",
        "\n",
        "# --- AGENT ---\n",
        "class SimpleAgent:\n",
        "    def __init__(self, model, tools, system=\"\"):\n",
        "        self.system = system\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "        # Build graph\n",
        "        g = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes (steps)\n",
        "        g.add_node(\"llm\", self.ask_model)\n",
        "        g.add_node(\"action\", self.run_tool)\n",
        "\n",
        "        # Add conditional edges\n",
        "        g.add_conditional_edges(\n",
        "            \"llm\",\n",
        "            self.should_use_tool,  # function that decides next step\n",
        "            {True: \"action\", False: END}\n",
        "        )\n",
        "\n",
        "        # After using tool, go back to the LLM\n",
        "        g.add_edge(\"action\", \"llm\")\n",
        "\n",
        "        # Start at the LLM node\n",
        "        g.set_entry_point(\"llm\")\n",
        "        self.graph = g.compile()\n",
        "\n",
        "    # --- Decide whether to use a tool ---\n",
        "    def should_use_tool(self, state: AgentState):\n",
        "        last_msg = state[\"messages\"][-1]\n",
        "        return len(last_msg.tool_calls) > 0\n",
        "\n",
        "    # --- Step 1: Ask the LLM what to do ---\n",
        "    def ask_model(self, state: AgentState):\n",
        "        msgs = [SystemMessage(content=self.system)] + state[\"messages\"]\n",
        "        reply = self.model.invoke(msgs)\n",
        "        return {\"messages\": [reply]}\n",
        "\n",
        "    # --- Step 2: Run tool if needed ---\n",
        "    def run_tool(self, state: AgentState):\n",
        "        tool_calls = state[\"messages\"][-1].tool_calls\n",
        "        results = []\n",
        "        for t in tool_calls:\n",
        "            tool_name = t[\"name\"]\n",
        "            args = t[\"args\"]\n",
        "            if tool_name in self.tools:\n",
        "                result = self.tools[tool_name].invoke(args)\n",
        "            else:\n",
        "                result = \"Unknown tool ‚Äî please retry.\"\n",
        "            results.append(ToolMessage(\n",
        "                tool_call_id=t[\"id\"], name=tool_name, content=str(result)\n",
        "            ))\n",
        "        return {\"messages\": results}\n",
        "\n",
        "# --- SYSTEM PROMPT ---\n",
        "prompt = \"\"\"You are a helpful assistant.\n",
        "If someone asks a question that requires information, use the search_tool.\n",
        "Otherwise, answer directly.\"\"\"\n",
        "\n",
        "# --- RUN THE AGENT ---\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "agent = SimpleAgent(model, [tool], system=prompt)\n",
        "\n",
        "# Example input\n",
        "msgs = [HumanMessage(content=\"Search for the capital of France\")]\n",
        "result = agent.graph.invoke({\"messages\": msgs})\n",
        "\n",
        "print(\"\\n--- Final Response ---\")\n",
        "print(result[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "id": "w8sUodpGMNHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AGENTIC SEARCH TOOL ( Tavily - a search engine designed for llm where we search something it gets ul scraps and returns the answer instead of url ( like duckduckgo ))"
      ],
      "metadata": {
        "id": "VzVxVXNoNrYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß© Simple Tavily Search Example\n",
        "# Learn what Tavily is and how to use it\n",
        "\n",
        "from tavily import TavilyClient\n",
        "import os\n",
        "\n",
        "# 1Ô∏è‚É£ Setup: create a Tavily client with your API key\n",
        "# (you can store TAVILY_API_KEY in your .env file)\n",
        "client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
        "\n",
        "# 2Ô∏è‚É£ Run a simple query\n",
        "query = \"What is special about Nvidia‚Äôs new Blackwell GPU?\"\n",
        "result = client.search(query, include_answer=True)\n",
        "\n",
        "# 3Ô∏è‚É£ Print the short, summarized answer\n",
        "print(\"üîç Tavily Answer:\")\n",
        "print(result[\"answer\"])\n",
        "\n",
        "#example: ( if we ussed duckduckgo we will only get urls not answers or content summary)\n",
        "# {\n",
        "#   \"answer\": \"Nvidia's new Blackwell GPU offers up to 20x AI performance improvement...\",\n",
        "#   \"results\": [\n",
        "#     {\n",
        "#       \"title\": \"Nvidia Blackwell Overview\",\n",
        "#       \"url\": \"https://www.nvidia.com/en-us/data-center/blackwell-architecture/\",\n",
        "#       \"content\": \"Blackwell architecture introduces ...\"\n",
        "#     }\n",
        "#   ]\n",
        "# }\n",
        "\n",
        "\n",
        "# to use as tool for langgraph agents:\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "tool = TavilySearchResults(max_results=4)  # increased number of results ( if itself whill use api key fron .env no need to create a client )\n"
      ],
      "metadata": {
        "id": "PIyY_TnlNrWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LANGGRAPH PERSISTENCE AND STREAMING ( Persistance means = especially when useing long running agents we keep a state and we use over tie sometimes we go back to that state if we need again in future, happens by saving the state in a seperate database AND Streaming means = when agents are taking too long we output a stream of data which describes what exactly is happening inside llms )"
      ],
      "metadata": {
        "id": "w6hqjIAvO1DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERSISTANCE"
      ],
      "metadata": {
        "id": "3W9vt9GVO-uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Persistence Example ---\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "# Define simple memory for persistence\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "\n",
        "# Define a simple state type\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "\n",
        "# Simple echo node that repeats what the user says\n",
        "def echo_node(state: ChatState):\n",
        "    user_msg = state[\"messages\"][-1].content\n",
        "    reply = f\"Echo: {user_msg}\"\n",
        "    return {\"messages\": [AIMessage(content=reply)]}\n",
        "\n",
        "# Build graph\n",
        "graph = StateGraph(ChatState)\n",
        "graph.add_node(\"echo\", echo_node)\n",
        "graph.set_entry_point(\"echo\")\n",
        "bot = graph.compile(checkpointer=memory)\n",
        "\n",
        "# Simulate persistent conversation (same thread_id keeps memory)\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "for event in bot.stream({\"messages\": [HumanMessage(content=\"Hello!\")]}, thread):\n",
        "    print(event)\n",
        "\n",
        "for event in bot.stream({\"messages\": [HumanMessage(content=\"How are you?\")]}, thread):\n",
        "    print(event)\n",
        "\n",
        "# Even though the code runs in two separate calls, the graph ‚Äúremembers‚Äù the conversation because of the SQLite checkpoint (persistence)."
      ],
      "metadata": {
        "id": "Fqe9BHk_QryP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STREAMING"
      ],
      "metadata": {
        "id": "HrL_zajZQ3a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Streaming Example (fixed & clearer) ---\n",
        "import asyncio\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
        "from langchain_core.messages import HumanMessage\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "# Async checkpoint (not really used here, but shows concept of persistence)\n",
        "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
        "\n",
        "# Define simple conversation state\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "\n",
        "# A node that streams output word by word\n",
        "async def stream_node(state: ChatState):\n",
        "    user_text = state[\"messages\"][-1].content\n",
        "    # Simulate model streaming one word at a time\n",
        "    for word in f\"Streaming reply to: {user_text}\".split():\n",
        "        await asyncio.sleep(0.1)  # simulate delay\n",
        "        yield {\"event\": \"on_chat_model_stream\", \"data\": {\"chunk\": {\"content\": word + \" \"}}}\n",
        "\n",
        "# Build simple graph\n",
        "graph = StateGraph(ChatState)\n",
        "graph.add_node(\"stream\", stream_node)\n",
        "graph.set_entry_point(\"stream\")\n",
        "bot = graph.compile(checkpointer=memory)\n",
        "\n",
        "async def main():\n",
        "    thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "    stream = []  # <-- we collect chunks here\n",
        "\n",
        "    async for event in bot.astream_events({\"messages\": [HumanMessage(content=\"Hi there!\")]}, thread):\n",
        "        if event[\"event\"] == \"on_chat_model_stream\":\n",
        "            chunk = event[\"data\"][\"chunk\"][\"content\"]\n",
        "            print(chunk, end=\"\")  # live stream output\n",
        "            stream.append(chunk)  # store chunk\n",
        "\n",
        "    print(\"\\n\\nFull streamed message:\")\n",
        "    print(\"\".join(stream))  # <-- final combined output\n",
        "\n",
        "asyncio.run(main())\n",
        "\n",
        "# Instead of printing a full response at once, the assistant ‚Äústreams‚Äù it piece by piece (word by word), like a live typing effect.\n",
        "\n",
        "# Each event from astream_events() yields a partial output chunk that we print in real time to simulate streaming.\n",
        "# We also store these chunks in a list and join them afterward to form the complete message."
      ],
      "metadata": {
        "id": "Gz62vYy5Q4j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LANGGRAPH HUMAN IN THE LOOP"
      ],
      "metadata": {
        "id": "sApceyQsdJK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MANUAL HUMAN IN LOOP"
      ],
      "metadata": {
        "id": "SQZYV18hAoLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEPS: UPDATE STATE ARRAY CODE TO BE THREADID AWARE AND WHEN WE WILL PROCEED THE NEXT STATE WE USE HUMAN PERMISSIN LIKE TYPING YES IN DIALOGUE BOX TO PROCEED WITH IT'S DECISION\n",
        "# STATE IS A SNAPSHOT OF CURRENT NODE AND ALL IT'S METADATA, BY OBSERVING STATE ARAY WE CAN UNDERSAND WHAT HAS BEEN PROCESSED IN WHAT NODES\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step-1: import dependencies\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from uuid import uuid4\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "\n",
        "# Step-2: Upgraded state array logic\n",
        "\"\"\"\n",
        "In previous examples we've annotated the `messages` state key\n",
        "with the default `operator.add` or `+` reducer, which always\n",
        "appends new messages to the end of the existing messages array.\n",
        "\n",
        "Now, to support replacing existing messages, we annotate the\n",
        "`messages` key with a customer reducer function, which replaces\n",
        "messages with the same `id`, and appends them otherwise.\n",
        "\"\"\"\n",
        "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
        "    # assign ids to messages that don't have them\n",
        "    for message in right:\n",
        "        if not message.id:\n",
        "            message.id = str(uuid4())\n",
        "    # merge the new messages with the existing messages\n",
        "    merged = left.copy()\n",
        "    for message in right:\n",
        "        for i, existing in enumerate(merged):\n",
        "            # replace any existing messages with the same id\n",
        "            if existing.id == message.id:\n",
        "                merged[i] = message\n",
        "                break\n",
        "        else:\n",
        "            # append any new messages to the end\n",
        "            merged.append(message)\n",
        "    return merged\n",
        "\n",
        "\n",
        "# Step-3: Broiler plate code about langgraph agent, except add a new parameter in below graph.compile with interrupt bfore where we cause interuption\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], reduce_messages]\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
        "        self.system = system\n",
        "        graph = StateGraph(AgentState)\n",
        "        graph.add_node(\"llm\", self.call_openai)\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "        graph.set_entry_point(\"llm\")\n",
        "        self.graph = graph.compile(\n",
        "            checkpointer=checkpointer,\n",
        "            interrupt_before=[\"action\"]\n",
        "        )\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "    def call_openai(self, state: AgentState):\n",
        "        messages = state['messages']\n",
        "        if self.system:\n",
        "            messages = [SystemMessage(content=self.system)] + messages\n",
        "        message = self.model.invoke(messages)\n",
        "        return {'messages': [message]}\n",
        "\n",
        "    def exists_action(self, state: AgentState):\n",
        "        print(state)\n",
        "        result = state['messages'][-1]\n",
        "        return len(result.tool_calls) > 0\n",
        "\n",
        "    def take_action(self, state: AgentState):\n",
        "        tool_calls = state['messages'][-1].tool_calls\n",
        "        results = []\n",
        "        for t in tool_calls:\n",
        "            print(f\"Calling: {t}\")\n",
        "            result = self.tools[t['name']].invoke(t['args'])\n",
        "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
        "        print(\"Back to the model!\")\n",
        "        return {'messages': results}\n",
        "\n",
        "\n",
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "abot = Agent(model, [tool], system=prompt, checkpointer=memory) # <--- Agent initialization\n",
        "\n",
        "# runs a query (\"Whats the weather in SF?\") through your LangGraph agent, using the defined thread for persistence, and prints the agent's process and final output as it happens.\n",
        "messages = [HumanMessage(content=\"Whats the weather in SF?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread): # triggers graph execution = abot.graph.stream() initiates the LangGraph execution with the given state and thread (using.stream() function), yielding events as the agent processes the query.\n",
        "    for v in event.values(): #  iterates through and prints agent events, revealing the agent's internal steps and outputs.\n",
        "        print(v)\n",
        "\n",
        "\n",
        "# Analyse states to manual node process histry analysis\n",
        "abot.graph.get_state(thread)\n",
        "abot.graph.get_state(thread).next\n",
        "\n",
        "# Step-4: Code to human for manual interaction from terminal to type y to continue with processing:\n",
        "\n",
        "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n",
        "while abot.graph.get_state(thread).next:\n",
        "    print(\"\\n\", abot.graph.get_state(thread),\"\\n\")\n",
        "    _input = input(\"proceed?\")\n",
        "    if _input != \"y\":\n",
        "        print(\"aborting\")\n",
        "        break\n",
        "    for event in abot.graph.stream(None, thread):\n",
        "        for v in event.values():\n",
        "            print(v)"
      ],
      "metadata": {
        "id": "YlBaBeNk9Nf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODIFY STATE ( in state array as we store node state and metadata as processing history, we can also edit the state array and restart it from particular state simulation agent's modifed or wild card processing )"
      ],
      "metadata": {
        "id": "mnP3-hp0BRwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continution of above code cell: where we run until the interrupt and then modify the state\n",
        "\n",
        "# rerun the graph with a new thread id so we don't conflict memeory state:\n",
        "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n",
        "\n",
        "# Analyse States\n",
        "\n",
        "abot.graph.get_state(thread)\n",
        "current_values = abot.graph.get_state(thread)\n",
        "current_values.values['messages'][-1]\n",
        "current_values.values['messages'][-1].tool_calls\n",
        "\n",
        "# modify state\n",
        "_id = current_values.values['messages'][-1].tool_calls[0]['id']\n",
        "current_values.values['messages'][-1].tool_calls = [\n",
        "    {'name': 'tavily_search_results_json',\n",
        "  'args': {'query': 'current weather in Louisiana'},\n",
        "  'id': _id}\n",
        "]\n",
        "\n",
        "abot.graph.update_state(thread, current_values.values)\n",
        "\n",
        "# analyse updated values\n",
        "abot.graph.get_state(thread)\n",
        "\n",
        "# re-run the graph from that particular point:\n",
        "for event in abot.graph.stream(None, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n"
      ],
      "metadata": {
        "id": "SreOs4tDBlFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TIME-TRAVEL ( when we go behind some state in state memory and re-run it from any prev state it's called time-travel )"
      ],
      "metadata": {
        "id": "yuskSRPwDVZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# == First Time-travel concept: Go back in time\n",
        "\n",
        "# print state history and append in a list\n",
        "states = []\n",
        "for state in abot.graph.get_state_history(thread):\n",
        "    print(state)\n",
        "    print('--')\n",
        "    states.append(state)\n",
        "\n",
        "# go-back 3 states\n",
        "to_replay = states[-3]\n",
        "\n",
        "# start again from 3 prev states\n",
        "for event in abot.graph.stream(None, to_replay.config):\n",
        "    for k, v in event.items():\n",
        "        print(v)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# == Second Time-travel concept: Go back in time and edit\n",
        "\n",
        "to_replay\n",
        "\n",
        "# update to_replay value\n",
        "_id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
        "to_replay.values['messages'][-1].tool_calls = [{'name': 'tavily_search_results_json',\n",
        "  'args': {'query': 'current weather in LA, accuweather'},\n",
        "  'id': _id}]\n",
        "\n",
        "# update state\n",
        "branch_state = abot.graph.update_state(to_replay.config, to_replay.values)\n",
        "\n",
        "# re-run the graph\n",
        "for event in abot.graph.stream(None, branch_state):\n",
        "    for k, v in event.items():\n",
        "        if k != \"__end__\":\n",
        "            print(v)\n",
        "\n",
        "\n",
        "# == Third Time-travel concept: Add message to a state at a given time\n",
        "\n",
        "# get the state\n",
        "to_replay\n",
        "_id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
        "\n",
        "# modify the state\n",
        "state_update = {\"messages\": [ToolMessage(\n",
        "    tool_call_id=_id,\n",
        "    name=\"tavily_search_results_json\",\n",
        "    content=\"54 degree celcius\",\n",
        ")]}\n",
        "\n",
        "# create modified state\n",
        "branch_and_add = abot.graph.update_state(\n",
        "    to_replay.config,\n",
        "    state_update,\n",
        "    as_node=\"action\")\n",
        "\n",
        "# rerun the graph\n",
        "for event in abot.graph.stream(None, branch_and_add):\n",
        "    for k, v in event.items():\n",
        "        print(v)"
      ],
      "metadata": {
        "id": "d1nMUD5UDfaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FURTHER RESEARCH CONCEPTS ( LANGSMITH, LANGSERVE)"
      ],
      "metadata": {
        "id": "18LyCmG9RkfX"
      }
    }
  ]
}